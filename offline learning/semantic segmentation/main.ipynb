{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamnaanaa/opt/anaconda3/envs/pl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataloader.dataloader import DTSegmentationDataset\n",
    "from model.model import DTSegmentationNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 100, val size: 25\n"
     ]
    }
   ],
   "source": [
    "dataset = DTSegmentationDataset()\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train, val = random_split(dataset, [train_size, test_size])\n",
    "print(f\"Train size: {len(train)}, val size: {len(val)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the distribution of the labels in the whole dataset for weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels distribution: [0.19483307 0.03665143 0.68552852 0.08298698 0.         0.\n",
      " 0.         0.        ]\n",
      "Train labels weights: [ 2.56629941 13.64203167  0.72936426  6.02504158         inf         inf\n",
      "         inf         inf]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/cgck8dhs36lfp446smw_gy7m0000gn/T/ipykernel_56234/3631225557.py:8: RuntimeWarning: divide by zero encountered in divide\n",
      "  weights = np.sum(labels) / (2 * labels)\n"
     ]
    }
   ],
   "source": [
    "# These values will be used in the model to compute the weighted loss\n",
    "labels = np.zeros(len(DTSegmentationDataset.SEGM_LABELS))\n",
    "for img, target in dataset:\n",
    "    for label, label_info in DTSegmentationDataset.SEGM_LABELS.items():\n",
    "        labels[label_info['id']] += (target == label_info['id']).sum()\n",
    "labels = labels / np.sum(labels)\n",
    "print(f\"Train labels distribution: {labels}\")\n",
    "weights = np.sum(labels) / (2 * labels)\n",
    "print(f\"Train labels weights: {weights}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_hparams = {\n",
    "    # --- Model ---\n",
    "    # | Model hyperparameters\n",
    "    'num_classes': 2,\n",
    "    # | Optimization hyperparameters\n",
    "    \"learning_rate\": 0.0625,\n",
    "    \"weight_decay\": 0.000000625,\n",
    "    \"lr_decay\": 0.25,\n",
    "    \n",
    "    # --- Dataloader (Hardware-specific) ---\n",
    "    \"batch_size\": 12,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "\n",
    "model = DTSegmentationNetwork(overfit_hparams)\n",
    "\n",
    "# Overfit for testing\n",
    "early_stop_overfit_callback = EarlyStopping(\n",
    "    monitor='loss',\n",
    "    patience=150,\n",
    "    min_delta=0.0005,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    log_every_n_steps=1,\n",
    "    max_epochs=150,\n",
    "    overfit_batches=1,\n",
    "    callbacks=[early_stop_overfit_callback]\n",
    ")\n",
    "\n",
    "# image, target = train[0]\n",
    "# # print(f'image: {image.shape}, target: {target.shape}')\n",
    "# result = torch.argmax(target, dim=0)\n",
    "# print(f'image: {image.shape}, result: {result.shape}, value range: {torch.min(result)}-{torch.max(result)}')\n",
    "\n",
    "trainer.fit(model, DataLoader(train, shuffle=False, batch_size=1), DataLoader(val, shuffle=False, batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the overfitted results\n",
    "img, target = train[0]\n",
    "print(f\"Target shape: {target.shape}, value range: {torch.min(target)}-{torch.max(target)}\")\n",
    "result = torch.argmax(model(img.unsqueeze(0)), dim=1)\n",
    "print(f\"Result shape: {result.shape}, value range: {torch.min(result)}-{torch.max(result)}\")\n",
    "pil_transform = transforms.ToPILImage()\n",
    "pil_transform(img).show(title=\"image\")\n",
    "pil_transform(DTSegmentationDataset.label_img_to_rgb(target)).show(title=\"ground truth\")\n",
    "pil_transform(DTSegmentationDataset.label_img_to_rgb(result[0])).show(title=\"prediction\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | LRASPP | 3.2 M \n",
      "---------------------------------\n",
      "246 K     Trainable params\n",
      "3.0 M     Non-trainable params\n",
      "3.2 M     Total params\n",
      "12.875    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 9/9 [00:26<00:00,  2.95s/it, loss=0.883, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 9/9 [00:45<00:00,  5.09s/it, loss=0.646, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.198 >= min_delta = 0.01. New best score: 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 9/9 [00:45<00:00,  5.09s/it, loss=0.487, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.015 >= min_delta = 0.01. New best score: 0.485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 9/9 [00:48<00:00,  5.44s/it, loss=0.277, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.035 >= min_delta = 0.01. New best score: 0.450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 9/9 [00:46<00:00,  5.14s/it, loss=0.24, v_num=3] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.053 >= min_delta = 0.01. New best score: 0.398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 9/9 [00:46<00:00,  5.16s/it, loss=0.227, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.073 >= min_delta = 0.01. New best score: 0.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 9/9 [00:45<00:00,  5.08s/it, loss=0.203, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.058 >= min_delta = 0.01. New best score: 0.267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 9/9 [00:45<00:00,  5.09s/it, loss=0.186, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.033 >= min_delta = 0.01. New best score: 0.233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 9/9 [00:46<00:00,  5.14s/it, loss=0.184, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.01. New best score: 0.223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 9/9 [00:45<00:00,  5.08s/it, loss=0.188, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.012 >= min_delta = 0.01. New best score: 0.210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 9/9 [00:45<00:00,  5.08s/it, loss=0.187, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.013 >= min_delta = 0.01. New best score: 0.198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 9/9 [00:45<00:00,  5.10s/it, loss=0.176, v_num=3]Epoch 00026: reducing learning rate of group 0 to 1.5625e-02.\n",
      "Epoch 28: 100%|██████████| 9/9 [00:45<00:00,  5.07s/it, loss=0.161, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 7 records. Best score: 0.198. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 9/9 [00:45<00:00,  5.08s/it, loss=0.161, v_num=3]\n",
      "Saving model... model.pt\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    # --- Model ---\n",
    "    # | Model hyperparameters\n",
    "    'num_classes': 4,\n",
    "    # | Optimization hyperparameters\n",
    "    \"learning_rate\": 0.0625,\n",
    "    \"weight_decay\": 0.000000625,\n",
    "    \"lr_decay\": 0.25,\n",
    "    \n",
    "    # --- Dataloader (Hardware-specific) ---\n",
    "    \"batch_size\": 16,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "# Initialize a tensor with pre-computed weights for the weighted loss: \n",
    "# * no crop, 5 classes - (bg, mid, road, end, side): UNKNOWN, wrong values:[1.04134481, 20.79269895, 1.1516967, 50.6744693, 9.6533002]\n",
    "# * 30% crop, 4 classes - (bg, mid, road, side): [2.56629941, 13.64203167,  0.72936426, 6.02504158]\n",
    "class_weights = torch.tensor([2.56629941, 13.64203167,  0.72936426, 6.02504158], dtype=torch.float32)\n",
    "hparams['class_weights'] = class_weights\n",
    "\n",
    "model = DTSegmentationNetwork(hparams)\n",
    "\n",
    "# Training procedure\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    min_delta=0.01,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    log_every_n_steps=1,\n",
    "    # For new MacBooks\n",
    "    # accelerator=\"mps\",\n",
    "    # devices=1,\n",
    "    callbacks=[early_stop_callback]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], shuffle=True)\n",
    "val_dataloader = DataLoader(val, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'])\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "model.save(\"model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the state dict of the model\n",
    "torch.save(model.state_dict(), \"model_v7_0_086_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected 1D vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline learning/semantic segmentation/main.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m# Find a lane that is closest to the middle of the image\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     closest_lane \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin(np\u001b[39m.\u001b[39mabs(x \u001b[39m-\u001b[39m IMAGE_WIDTH \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     m, b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mpolyfit(x[closest_lane], y[closest_lane], \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     drawer\u001b[39m.\u001b[39mline((\u001b[39m0\u001b[39m, b, IMAGE_WIDTH, m \u001b[39m*\u001b[39m IMAGE_WIDTH \u001b[39m+\u001b[39m b), fill\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m, width\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# # Fit a line to the side lane pixels and draw it on the prediction and the target\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# side_lane_pixels = torch.nonzero(target == DTSegmentationDataset.SEGM_LABELS['Side Lane']['id'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# if len(side_lane_pixels) > 0:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hamnaanaa/Projects/Duckietown/DT-autonomous-agent/offline%20learning/semantic%20segmentation/main.ipynb#X16sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Add the prediction to the final image\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mpolyfit\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pl/lib/python3.10/site-packages/numpy/lib/polynomial.py:636\u001b[0m, in \u001b[0;36mpolyfit\u001b[0;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected deg >= 0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 636\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected 1D vector for x\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    638\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected non-empty vector for x\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected 1D vector for x"
     ]
    }
   ],
   "source": [
    "# visualize the model validation results\n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 288\n",
    "PICS_PER_ROW = 3\n",
    "NUM_IMG_SIDE_BY_SIDE = 3\n",
    "\n",
    "# Generate a grid image (with PICS_PER_ROW images per row) that has all validation targets and the predictions for corresponding images side-by-side\n",
    "final_img = Image.new('RGB', (IMAGE_WIDTH * PICS_PER_ROW * NUM_IMG_SIDE_BY_SIDE, IMAGE_HEIGHT * (len(val) // PICS_PER_ROW + 1)))\n",
    "\n",
    "\n",
    "for index, (img, target) in enumerate(val):\n",
    "    # print(f\"Target shape: {target.shape}, value range: {torch.min(target)}-{torch.max(target)}\")\n",
    "    result = torch.argmax(model(img.unsqueeze(0)), dim=1)\n",
    "    # print(f\"Result shape: {result.shape}, value range: {torch.min(result)}-{torch.max(result)}\")\n",
    "    pil_transform = transforms.ToPILImage()\n",
    "    img_pil = pil_transform(img)\n",
    "    \n",
    "    # Add the image to the final image\n",
    "    width_offset = (IMAGE_WIDTH * NUM_IMG_SIDE_BY_SIDE) * (index % PICS_PER_ROW)\n",
    "    height_offset = IMAGE_HEIGHT * (index // PICS_PER_ROW)\n",
    "    final_img.paste(img_pil, (width_offset, height_offset))\n",
    "    \n",
    "    pil_target = pil_transform(DTSegmentationDataset.label_img_to_rgb(target))\n",
    "    \n",
    "    # Add the target to the final image\n",
    "    width_offset = (IMAGE_WIDTH * NUM_IMG_SIDE_BY_SIDE) * (index % PICS_PER_ROW) + IMAGE_WIDTH\n",
    "    height_offset = IMAGE_HEIGHT * (index // PICS_PER_ROW)\n",
    "    final_img.paste(pil_target, (width_offset, height_offset))\n",
    "    \n",
    "    pil_prediction = pil_transform(DTSegmentationDataset.label_img_to_rgb(result[0]))\n",
    "    \n",
    "    # Identify two lines in the image: the middle lane and the side lane using Hough transform\n",
    "    \n",
    "    drawer = ImageDraw.Draw(pil_prediction)\n",
    "    # Fit a line to the middle lane pixels and draw it on the prediction and the target\n",
    "    middle_lane_pixels = torch.nonzero(target == DTSegmentationDataset.SEGM_LABELS['Middle Lane']['id'])\n",
    "    if len(middle_lane_pixels) > 0:\n",
    "        x = middle_lane_pixels[:, 1]\n",
    "        y = middle_lane_pixels[:, 0]\n",
    "        # Find the mass center for the middle lane pixels\n",
    "        mass_center = (int(np.mean(x)), int(np.mean(y)))\n",
    "        # Draw \n",
    "        \n",
    "\n",
    "        # drawer.line((0, b, IMAGE_WIDTH, m * IMAGE_WIDTH + b), fill='red', width=10)\n",
    "    # # Fit a line to the side lane pixels and draw it on the prediction and the target\n",
    "    # side_lane_pixels = torch.nonzero(target == DTSegmentationDataset.SEGM_LABELS['Side Lane']['id'])\n",
    "    # if len(side_lane_pixels) > 0:\n",
    "    #     x = side_lane_pixels[:, 1]\n",
    "    #     y = side_lane_pixels[:, 0]\n",
    "    #     m, b = np.polyfit(x, y, 1)\n",
    "    #     drawer.line((0, b, IMAGE_WIDTH, m * IMAGE_WIDTH + b), fill='black', width=10)\n",
    "    \n",
    "\n",
    "    # Add the prediction to the final image\n",
    "    width_offset = (IMAGE_WIDTH * NUM_IMG_SIDE_BY_SIDE) * (index % PICS_PER_ROW) + (IMAGE_WIDTH * 2)\n",
    "    height_offset = IMAGE_HEIGHT * (index // PICS_PER_ROW)\n",
    "    final_img.paste(pil_prediction, (width_offset, height_offset))\n",
    "    \n",
    "final_img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the prediction to focus on the lower part of the image\n",
    "rgb_prediction = rgb_prediction[rgb_prediction.shape[0] // 2:, :, :]\n",
    "pil_prediction = pil_transform(rgb_prediction)\n",
    "\n",
    "# Find the average coordinate of the road\n",
    "road_mask = np.argwhere(rgb_prediction == DTSegmentationDataset.SEGM_LABELS['Ego Lane']['rgb_value'])\n",
    "road_center_y, road_center_x = np.ceil(np.mean(road_mask, axis=0)[:2])\n",
    "\n",
    "drawer = ImageDraw.Draw(pil_prediction)\n",
    "# Draw two circles (bigger and smaller) centered on road center coordinate\n",
    "RADIUS = 20\n",
    "drawer.ellipse((road_center_x - RADIUS, road_center_y - RADIUS, road_center_x + RADIUS, road_center_y + RADIUS), fill='green')\n",
    "drawer.ellipse((road_center_x - 5, road_center_y - 5, road_center_x + 5, road_center_y + 5), fill='orange')\n",
    "# Draw a vertical line on the center of the image\n",
    "drawer.line((pil_prediction.width / 2, 0, pil_prediction.width / 2, pil_prediction.height), fill='red')\n",
    "# Draw a horizontal line from the center of the image to the road center\n",
    "drawer.line((pil_prediction.width / 2, road_center_y, road_center_x, road_center_y), fill='blue')\n",
    "# Draw two lines going from the corners of the image to the road center\n",
    "drawer.line((0, 0, road_center_x, road_center_y), fill='yellow', width=2)\n",
    "drawer.line((pil_prediction.width, 0, road_center_x, road_center_y), fill='yellow', width=2)\n",
    "\n",
    "pil_prediction.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize the test image\n",
    "img = transforms.ToTensor()(Image.open(\"offline learning/semantic segmentation/data/frames_test/track_test_6.png\"))\n",
    "print(f\"Image shape: {img.size}\")\n",
    "result = torch.argmax(model(img.unsqueeze(0)), dim=1)\n",
    "print(f\"Result shape: {result.shape}, value range: {torch.min(result)}-{torch.max(result)}\")\n",
    "pil_transform = transforms.ToPILImage()\n",
    "pil_transform(img).show(title=\"image\")\n",
    "rgb_prediction = DTSegmentationDataset.label_img_to_rgb(result[0])\n",
    "pil_prediction = pil_transform(rgb_prediction)\n",
    "pil_prediction.show(title=\"prediction\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "time_start = time.time()\n",
    "r = requests.get('http://192.168.0.108:1318/image')\n",
    "time_request = time.time() - time_start\n",
    "pil_image = Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n",
    "image = transforms.ToTensor()(pil_image)\n",
    "time_convert = time.time() - (time_request + time_start)\n",
    "\n",
    "pred = torch.argmax(model(image.unsqueeze(0)), dim=1)[0]\n",
    "time_prediction = time.time() - (time_convert + time_start)\n",
    "\n",
    "title = f\"{time.time()}\"\n",
    "pil_image.save(f\"testing/out/image_{title}.png\")\n",
    "pil_image.show(title=\"image\")\n",
    "\n",
    "pil_transform = transforms.ToPILImage()\n",
    "rgb_prediction = DTSegmentationDataset.label_img_to_rgb(pred)\n",
    "pil_prediction = pil_transform(rgb_prediction)\n",
    "pil_prediction.save(f\"testing/out/prediction_{title}.png\")\n",
    "pil_prediction.show(title=\"prediction\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Request time: {time_request:.3f}s\n",
    "Convert time: {time_convert:.3f}s\n",
    "Prediction time: {time_prediction:.3f}s\n",
    "Total time: {(time_request + time_convert + time_prediction):.3f}s\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3315f46222374d13566524152beecd401bb6de98e11336215c532ce4113572da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
